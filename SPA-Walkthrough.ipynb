{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ling409 Final Project: Spanish Group\n",
    "Min Bae, Iris Chen, Abby Higgins, Josephine Shih"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweak #1: Imperative accents\n",
    "\n",
    "This was our primary change to the baseline code. The only addition to the main code loop was the addition of this line on line 417:\n",
    "\n",
    "\n",
    "`outform = add_accent_to_third_syllable(outform, msd)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The `add_accent_to_third_syllable` function relies on two helper functions `count_syllables` and `has_accent`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word):\n",
    "    \"\"\"Count syllables in a Spanish word based on vowel sequences\"\"\"\n",
    "    vowels = 'aeiouáéíóúAEIOUÁÉÍÓÚ'\n",
    "    diphthongs = ['ai', 'ei', 'oi', 'ui', 'au', 'eu', 'ou', \n",
    "                  'ia', 'ie', 'io', 'iu', 'ua', 'ue', 'uo']\n",
    "    count = 0\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "        if word[i] in vowels:\n",
    "            if i < len(word) - 1 and word[i:i+2].lower() in diphthongs:\n",
    "                count += 1\n",
    "                i += 2\n",
    "            else:\n",
    "                count += 1\n",
    "                i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "    return count\n",
    "\n",
    "def has_accent(word):\n",
    "    \"\"\"Check if word already contains any Spanish accent marks\"\"\"\n",
    "    accent_vowels = 'áéíóú'\n",
    "    return any(char in accent_vowels for char in word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is how `add_accent_to_third_syllable` works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_accent_to_third_syllable(word, msd):\n",
    "    \"\"\"Add accent to the third syllable from the end if:\n",
    "    1. The word is imperative\n",
    "    2. The word doesn't already have an accent mark\n",
    "    3. The word is long enough\n",
    "    4. The word is not negative\n",
    "    5. The word is not INFM,2,PL\"\"\"\n",
    "    \n",
    "    if 'IMP' not in msd:  # Only process imperatives\n",
    "        return word\n",
    "    if 'NEG' in msd:  # Don't process negatives\n",
    "        return word\n",
    "    if 'INFM,2,PL' in msd: # Edge case that doesn't require an accent \n",
    "        return word\n",
    "    if has_accent(word):  # Skip if word already has an accent\n",
    "        return word\n",
    "    \n",
    "    vowels = 'aeiouAEIOU'\n",
    "    accent_map = {'a': 'á', 'e': 'é', 'i': 'í', 'o': 'ó', 'u': 'ú'}\n",
    "    \n",
    "    # Count syllables from the end\n",
    "    syllables = count_syllables(word)\n",
    "    if syllables < 3:  # Word is too short\n",
    "        return word\n",
    "    \n",
    "    # Find the third-to-last vowel\n",
    "    vowel_count = 0\n",
    "    vowel_positions = []\n",
    "    \n",
    "    for i, char in enumerate(word):\n",
    "        if char in vowels:\n",
    "            vowel_count += 1\n",
    "            vowel_positions.append(i)\n",
    "    \n",
    "    if len(vowel_positions) >= 3:\n",
    "        # Get the position of the third-to-last vowel\n",
    "        target_pos = vowel_positions[-3]\n",
    "        # Add accent to that vowel\n",
    "        word_list = list(word)\n",
    "        if word_list[target_pos] in accent_map:\n",
    "            word_list[target_pos] = accent_map[word_list[target_pos]]\n",
    "        return ''.join(word_list)\n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how this changes `afrancesalas` to `afrancésalas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afrancésalas\n"
     ]
    }
   ],
   "source": [
    "print(add_accent_to_third_syllable(\"afrancesalas\", \"V;IMP;ACC(3,PL);NOM(INFM,2,SG\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of tweak #2, this tweak further improves the accuracy from 88.5% to 92.5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweak #2: Training data\n",
    "\n",
    "Besides the additions to the baseline code, we attempted to show the non-imperative issues with the training data. Some notable characteristics about the existing data sets:\n",
    "\n",
    "1) There are no overlaps of verbs between the .trn, .dev and .tst sets. This should have prevented any inflation of the prediction accuracy.\n",
    "\n",
    "2) Each verb is presented and tested with around/at least 20 msd forms. This means that a good chunk of the conjugation paradigm is given or predicted. The lines are distributed between the different tense-aspect-moods. (It is unclear what the criteria is for choosing specific msd's for each verb, since they differ between verbs. Part of it is due to the grammatical cases associated with each.)\n",
    "\n",
    "3) The vast majority of the data are -ar verbs. This matches the distribution of ar-er-ir in the actual Spanish language. (The RAE estimate at least 80% of verbs being of type -ar.) This leads to a the -er and -ir verbs being really underpresented and some of their special types being unrepresented. \n",
    "\n",
    "4) No highly irregular verbs are included (eg. ir, ser, estar, tener), though verbs that are irregular in ending (eg. subscribir) or stem changes (eg. dispertar) do appear. \n",
    "\n",
    "5) A large amount of the IMP data have an accusative or dative case, which is resulting in the issues with 3rd-to-last accents for most of the IMP data points. This is the aspect tried to fix with the added code functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following briefly explains the reasoning behind added lines to the training data, which attempt to address the issues from points 3 and 4 above.\n",
    "\n",
    "- We noticed (from modifying the baseline code to print out the chosen best rule) that a lot of the mistaken predictions are working off of rules with only 1 or 2 in frequency. The preferences for longer-replaced-suffix or -replacement-suffix also resulted in some strange outcomes, a lot of which could be fixed if there were just 1 (more) representation of the special verb type in the training data. \n",
    "\n",
    "- So instead of trying to add 'whole' paradigms (because it is still unclear what the criteria for msd's are), we added one line to match for each false prediction from our .dev outcomes (which we analyzed via diff). This would minimize any unforeseen effects on the entire function while showing/fixing some of the deficiencies with what we are working with.\n",
    "\n",
    "- The following are categorized and listed by their appearance in the .dev set.\n",
    "\n",
    "- All the added verbs were chosen based on their similarity in type to the problem predictions and if they were a \"commonly-used\" verb (or at least one that is easily recognizable instead of obscure). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) sonreír\n",
    "\n",
    "- Because of the accent on the thematic vowel, this is a special type of verb. The function rightfully recognizes that as being different from -ir, but it resulted in having no replacement-suffix to go off of except for -r. This obviously led to bad predictions, since the data is largely dominated by -ar verbs and replacing just the -r doesn't deal with parts of the verb that actually needs to be conjugated.\n",
    "\n",
    "- Because there was no verb of this type in the .trn data, we added in lines for desleír, using the msd's of sonreír. (This is the only instant where we added a 'whole' paradigm.) Just having one instance fixed the predicitons upon re-running the code, showing that the baseline has great potential to work with small data sets as long as the data is \"comprehensive\" in representing the Spanish langauge.\n",
    "\n",
    "```\n",
    "desleír\tV;COND;NOM(3,PL)\tdesleirían\n",
    "desleír\tV;IMP;ACC(1,PL);NOM(INFM,2,SG)\tdeslíenos\n",
    "desleír\tV;IMP;ACC(3,SG);NOM(FORM,2,SG)\tdeslíase\n",
    "desleír\tV;IMP;DAT(1,PL);NOM(1,PL)\tdesliámonos\n",
    "desleír\tV;IMP;DAT(2,PL);NOM(1,PL)\tdesliámoos\n",
    "desleír\tV;IMP;DAT(3,PL);NOM(INFM,2,SG)\tdeslíeles\n",
    "desleír\tV;IMP;DAT(3,SG);NOM(FORM,2,PL)\tdeslíanle\n",
    "desleír\tV;IND;PRS;NOM(1,PL)\tdesleímos\n",
    "desleír\tV;IND;PST;IPFV;NOM(1,PL)\tdesleíamos\n",
    "desleír\tV;IND;PST;PFV;NOM(2,SG,FORM)\tdeslió\n",
    "desleír\tV;NFIN;DAT(1,PL)\tdesleírnos\n",
    "desleír\tV;NFIN;DAT(2,SG)\tdesleírte\n",
    "desleír\tV;NFIN;DAT(3,PL)\tdesleírse\n",
    "desleír\tV;POS;IMP;NOM(1,PL)\tdesliamos\n",
    "desleír\tV;SBJV;FUT;NOM(3,PL)\tdeslieren\n",
    "desleír\tV;SBJV;PRS;NOM(2,PL)\tdesliáis\n",
    "desleír\tV;SBJV;PST;LGSPEC1;NOM(2,SG,INFM)\tdeslieras\n",
    "desleír\tV;SBJV;PST;LGSPEC2;NOM(1,PL)\tdesliésemos\n",
    "desleír\tV;SBJV;PST;LGSPEC2;NOM(2,SG,FORM)\tdesliese\n",
    "desleír\tV;SBJV;PST;LGSPEC2;NOM(3,SG)\tdesliese\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) subscribir and seducir\n",
    "\n",
    "- These have the sort of the same issue as sonreír, but on a smaller scale. While these irregular-ending types did have their type represented in the training data, specific msd's asked for in the .dev predictions were not (because the function that chooses the best rule filters by msd). So it resulted in them pulling from rules meant for the general -ir verbs.  \n",
    "\n",
    "- Like with desleír, we only added in the msd's that were asked to predict and it fixed it.\n",
    "\n",
    "```\n",
    "reinscribir\tV;V.PTCP;PST;FEM;SG\treinscripta\n",
    "traducir\tV;V.PTCP;PST;MASC;PL\ttraducidos\n",
    "traducir\tV;V.PTCP;PST;MASC;SG\ttraducido\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) prostituir\n",
    "\n",
    "- Because the training data contained just as many -guir and -uir verbs, the predictions were pulling from the verbs created by the -guir portion of the training data. This is one of the cases where the grapheme doesn't capture the changes in Spanish, since the 'u' in -guir is not pronounced and is used to make the 'g' a plosive, while in other -uir verbs it is pronounced as a separate vowel from the -ir.\n",
    "\n",
    "- Upon looking at the frequency of rules generated, we saw that just one more instance (since it was at a tie) of \"normal\" -uir verbs would have made the difference, since the rule-chosing function was originally choosing wrongly at the longest-replacement-suffix stage.\n",
    "\n",
    "```\n",
    "constituir\tV;SBJV;PRS;NOM(2,SG,INFM)\tconstituyás\n",
    "constituir\tV;SBJV;PRS;NOM(3,PL)\tconstituyan\n",
    "constituir\tV;SBJV;PST;LGSPEC2;NOM(1,SG)\tconstituyese\n",
    "constituir\tV;SBJV;PST;LGSPEC2;NOM(3,SG)\tconstituyese\n",
    "constituir\tV;V.CVB;PRS;ACC(3,PL)\tconstituyéndose\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tweak on its own improved the accuracy of the model from 86.1% to 88.5%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "409",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
